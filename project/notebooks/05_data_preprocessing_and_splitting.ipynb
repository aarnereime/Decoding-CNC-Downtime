{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to do common data preprocessing steps and splitting for our data. This data will later be used other notebooks where we do more specific preprocessing steps for models like logistic regression, and XGBoost and LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Making the project modules available in the notebook\n",
    "root = os.path.abspath(os.path.join('../..'))\n",
    "if root not in sys.path: sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(root, 'project/data/downtime_window_sequences.csv')\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group_id')['label'].unique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our pre-processing steps, we will perform the following in the order listed below:\n",
    "\n",
    "- Check for data leakage\n",
    "- Remove highly correlated features\n",
    "- Remove constant columns as they do not provide any information\n",
    "- Check for missing values\n",
    "- Remove features used to label the data, along with features hand picked by me that I believe are not useful\n",
    "- Split the data into training, validation, and test sets using a 70-15-15 split \n",
    "- After splitting the data, we will perform the following steps:\n",
    "    - Remove some features that are not useful for the model\n",
    "    - Impute missing values using the median value of the training set\n",
    "- Lastly, we will convert the data to numpy arrays and save the pre-processed data to for use in the next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to check for data leakage in our dataset, such as:\n",
    "- Overlapping windows was handleded in the downtime window extractor, but we need to confirm that this implementation actually worked.\n",
    "- Remove features that highly correlate with the features used to label the data. This is because if we use these features to train the model, it will learn to predict the labels based on these features instead of the actual features that we want to use.\n",
    "\n",
    "Later when we split the data, we will also make sure that the training, validation, and test sets do not overlap in time. This is important because we want to make sure that the model is not trained on data that it will see in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Written at a later stage:)* Earlier results during my masters resulted in excellent validation accuracy. They were so good that I was suspicious of data leakage. I did some investigations and concluded that there was no data leakage. However, as I was writing my report i realised that my tests was not thorough enough. I had missinterpreted the results of the test and drawn a false conclusion. I had to redo the test and found that there was indeed data leakage which I fixed in the downtime window extractor. To make sure that this worked, I will go through all data sample, retrieve the min and max timestamps and the machine id. I will then check if there are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_timestamp_machine_list = [] # list of tuples (min_timestamp, max_timestamp, machine_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_min_max_timestamp_per_machine(sub_df):\n",
    "    downtime_df = sub_df.loc[sub_df['status'] == 'downtime']\n",
    "    \n",
    "    min_timestamp = downtime_df['timestamp'].min()\n",
    "    max_timestamp = downtime_df['timestamp'].max()\n",
    "    machine_id = downtime_df['machine_id'].iloc[0]\n",
    "    \n",
    "    min_max_timestamp_machine_list.append((min_timestamp, max_timestamp, machine_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('group_id')\n",
    "grouped.apply(extract_min_max_timestamp_per_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_timestamp_machine_list_set = set(min_max_timestamp_machine_list)\n",
    "len(min_max_timestamp_machine_list_set) == len(min_max_timestamp_machine_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There are no overlapping windows in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for features correlated to the features used to label the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making the labels, we used the following features:\n",
    "- program\n",
    "- unitNum\n",
    "- PartCountAct\n",
    "- program_cmt\n",
    "- subprogram_cmt\n",
    "- Machine_state (not directly used, but should be checked)\n",
    "\n",
    "Even if we remove the first five features from the data, we still need to check if the other features are correlated to these features. We will do this by checking the correlation matrix and removing any features that are highly correlated with these features. We will use a threshold of 0.8 to remove features that are highly correlated with the features used to label the data.\n",
    "\n",
    "After doing that, we will also check if any columns are highly correlated to the label column. We will use the same threshold to remove features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_used_to_label_data = [\n",
    "    'Machine_state_machine',\n",
    "    'machine_shdr_program_Numeric',\n",
    "    'machine_shdr_unitNum_Numeric',\n",
    "    'machine_shdr_PartCountAct_Numeric',\n",
    "    'machine_shdr_program_cmt',\n",
    "    'machine_shdr_subprogram_cmt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leakage_df = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = leakage_df.select_dtypes(include=['object']).columns\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "leakage_df['Machine_state_machine'] = le.fit_transform(leakage_df['Machine_state_machine'].astype(str))\n",
    "leakage_df['machine_shdr_execution'] = le.fit_transform(leakage_df['machine_shdr_execution'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correlation(col_to_check, col_to_check_against):\n",
    "    df_num = leakage_df[col_to_check + col_to_check_against]\n",
    "    corr_all = df_num.corr()\n",
    "    corr_block = corr_all.loc[col_to_check, col_to_check_against]\n",
    "    max_corr = corr_block.abs().max(axis=1).sort_values(ascending=False)\n",
    "    \n",
    "    print(f'Top 10 correlations:')\n",
    "    print(max_corr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we check the correlation between features used to label the data\n",
    "feature_cols = [c for c in leakage_df.columns if c not in timestamps_used_to_label_data]\n",
    "\n",
    "numeric_feature_cols = leakage_df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_label_cols = leakage_df[timestamps_used_to_label_data].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "check_correlation(numeric_feature_cols, numeric_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondly, we will check the correlation between the features and the label\n",
    "leakage_df['label'] = le.fit_transform(leakage_df['label'].astype(str))\n",
    "\n",
    "feature_cols = [c for c in leakage_df.columns if c != 'label']\n",
    "numeric_feature_cols = leakage_df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "check_correlation(numeric_feature_cols, ['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be any features that are highly correlated with either the features used to label the data or the label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_columns = df.columns[df.nunique() == 1]\n",
    "constant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=constant_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().mean() * 100\n",
    "missing_values = missing_values.round(2)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features have a lot of missing values. I believe the cause of this is either that the feature was not measured at an earlier stage or that the machine simply does not use the feature (take Zpos as an example, even though the machines have access to this feature, they may not use it that often). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_missing_values_group = df.groupby('group_id').apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "mean_percentage_missing_values_group = percentage_missing_values_group.mean(axis=1).round(2)\n",
    "mean_percentage_missing_values_group_above_threshold = mean_percentage_missing_values_group[\n",
    "    mean_percentage_missing_values_group > 50].sort_values(ascending=False) \n",
    "\n",
    "mean_percentage_missing_values_group_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mean_percentage_missing_values_group_above_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "162 groups have about 80% of their data missing. This is such a high number that I will remove these groups from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_drop = mean_percentage_missing_values_group_above_threshold.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['group_id'].isin(groups_to_drop)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will run this code again to ensure that we have no more than 50% missing values in any group\n",
    "percentage_missing_values_group = df.groupby('group_id').apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "mean_percentage_missing_values_group = percentage_missing_values_group.mean(axis=1).round(2)\n",
    "mean_percentage_missing_values_group_above_threshold = mean_percentage_missing_values_group[\n",
    "    mean_percentage_missing_values_group > 50].sort_values(ascending=False) \n",
    "\n",
    "mean_percentage_missing_values_group_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group_id')['label'].unique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the groups with a lot of missing values, we get a label distribution that is slightly unbalanced. This needs to be taken into account when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [\n",
    "    # Features used to label the data\n",
    "    'machine_shdr_program_Numeric',\n",
    "    'machine_shdr_unitNum_Numeric',\n",
    "    'machine_shdr_PartCountAct_Numeric',\n",
    "    'machine_shdr_program_cmt',\n",
    "    'machine_shdr_subprogram_cmt',\n",
    "    \n",
    "    # Features I deemed to be not useful\n",
    "    'machine_shdr_subprogram_Numeric',\n",
    "    'machine_shdr_Tool_suffix',\n",
    "]\n",
    "\n",
    "df.drop(columns=features_to_drop, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some features that needs to be dropped, but these will be dropped later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the data, we will encode the target labels and transform the data to numpy arrays. We will not do this with the features yet because we need the feature names to process the data after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'planned': 0, 'unplanned': 1}\n",
    "df.loc[:, 'label'] = df['label'].map(label_mapping).astype(int)\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When splitting time series data into training, validation, and test sets, we need to make sure that the data is split in a way that preserves the temporal order of the data. This means that we cannot randomly split the data into training, validation, and test sets. Instead, we will use a time-based split where we will use the first 70% of the data for training, the next 15% for validation, and the last 15% for testing. This is an important step to prevent data leakage on time series data, because we want to make sure that the model is not trained on data that it will see in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(df, train_frac=0.7, val_frac=0.15):\n",
    "    test_frac = 1.0 - train_frac - val_frac\n",
    "    assert test_frac >= 0.0\n",
    "    assert train_frac + val_frac <= 1.0\n",
    "    assert train_frac + val_frac + test_frac == 1.0\n",
    "    print(f'Train: {train_frac}, Val: {val_frac}, Test: {test_frac:.2f}')\n",
    "    \n",
    "    df = df.sort_values('start_time')\n",
    "    \n",
    "    # STRATIFIED SPLIT\n",
    "    # train_parts, val_parts, test_parts = [], [], []\n",
    "    \n",
    "    # for label, group in df.groupby('label', sort=False):\n",
    "    #     n = len(group)\n",
    "    #     i_train = int(n * train_frac)\n",
    "    #     i_val = int(n * (train_frac + val_frac))\n",
    "        \n",
    "    #     train_parts.append(group.iloc[:i_train])\n",
    "    #     val_parts.append(group.iloc[i_train:i_val])\n",
    "    #     test_parts.append(group.iloc[i_val:])\n",
    "    \n",
    "    # train_df = pd.concat(train_parts).sort_values('start_time').reset_index(drop=True)\n",
    "    # val_df   = pd.concat(val_parts).sort_values('start_time').reset_index(drop=True)\n",
    "    # test_df  = pd.concat(test_parts).sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "    # NON-STRATIFIED SPLIT WITH NO OVERLAP\n",
    "    train_df = df.iloc[:int(len(df) * train_frac)]\n",
    "    val_df = df.iloc[int(len(df) * train_frac):int(len(df) * (train_frac + val_frac))]\n",
    "    test_df = df.iloc[int(len(df) * (train_frac + val_frac)):]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = (\n",
    "    df.groupby('group_id')\n",
    "    .agg(\n",
    "        start_time = ('timestamp', 'min'),\n",
    "        label = ('label', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "meta_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta, val_meta, test_meta = time_based_split(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.shape, val_meta.shape, test_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now split the data based on time, ensuring that the training, validation, and test sets do not overlap in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_meta['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a time-based split, we cannot do proper stratification of the label distribution without overlapping the data. This imbalance will need to be handled later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train_meta['group_id']\n",
    "val_ids = val_meta['group_id']\n",
    "test_ids = test_meta['group_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['group_id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = df[df['group_id'].isin(val_ids)].reset_index(drop=True)\n",
    "test_df = df[df['group_id'].isin(test_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([group.drop(columns=['label']) for _, group in train_df.groupby('group_id')], dtype=object)\n",
    "X_val = np.array([group.drop(columns=['label']) for _, group in val_df.groupby('group_id')], dtype=object)\n",
    "X_test = np.array([group.drop(columns=['label']) for _, group in test_df.groupby('group_id')], dtype=object)\n",
    "\n",
    "y_train = train_df.groupby('group_id')['label'].first().values.astype(int)\n",
    "y_val = val_df.groupby('group_id')['label'].first().values.astype(int)\n",
    "y_test = test_df.groupby('group_id')['label'].first().values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train size:      {len(X_train)}')\n",
    "print(f'Validation size: {len(X_val)}')\n",
    "print(f'Test size:       {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After we have splitted the data, we can remove the features that we do not need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_featrues(dataset_list):\n",
    "    features_to_remove = [\n",
    "        'machine_id',\n",
    "        'group_id',\n",
    "    ]\n",
    "    \n",
    "    dataset = [group.drop(columns=features_to_remove) for group in dataset_list]\n",
    "    return dataset\n",
    "\n",
    "print(f'X_train shape before: {X_train[0].shape}')\n",
    "X_train = remove_featrues(X_train)\n",
    "print(f'X_train shape after: {X_train[0].shape}')\n",
    "\n",
    "X_val = remove_featrues(X_val)\n",
    "X_test = remove_featrues(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we save our data we will impute missing values with the mean of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_before_impute = pd.concat(X_train, axis=0)\n",
    "print('Number of cells with missing values: ', X_train_concat_before_impute.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = X_train_concat_before_impute.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = X_train_concat_before_impute.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_imputer = SimpleImputer(strategy='mean').fit(X_train_concat_before_impute[numerical_columns])\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent').fit(X_train_concat_before_impute[categorical_columns])\n",
    "\n",
    "def impute_sequences(sub_df):\n",
    "    sub_df[numerical_columns] = numerical_imputer.transform(sub_df[numerical_columns])\n",
    "    sub_df[categorical_columns] = categorical_imputer.transform(sub_df[categorical_columns])\n",
    "    return sub_df\n",
    "\n",
    "X_train = [impute_sequences(group) for group in X_train]\n",
    "X_val = [impute_sequences(group) for group in X_val]\n",
    "X_test = [impute_sequences(group) for group in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_after_imputation = pd.concat(X_train, axis=0)\n",
    "print('Number of cells with missing values: ', X_train_concat_after_imputation.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to save our datasets, ready to be further processed in the notebooks for logistic regression and XGBoost preprocessing and LSTM preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(root, 'project/data/')\n",
    "\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_val': X_val,\n",
    "    'y_val': y_val,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test,\n",
    "    'numerical_imputer': numerical_imputer,\n",
    "    'catergorical_imputer': categorical_imputer\n",
    "}, data_folder + 'splitted_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
