{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - LSTM Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Making the project modules available in the notebook\n",
    "root = os.path.abspath(os.path.join('../..'))\n",
    "if root not in sys.path: sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(os.path.join(root, 'project/data/splitted_data.pt'))\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_dict['X_train']\n",
    "X_val = data_dict['X_val']\n",
    "X_test = data_dict['X_test']\n",
    "\n",
    "y_train = data_dict['y_train']\n",
    "y_val = data_dict['y_val']\n",
    "y_test = data_dict['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start of by encoding the feature `timestamp`. Since we plan on using LSTM models, we will encode the timestamp in cyclical format. We do this since we have timeseries data and we want to capture the cyclical behaviour of time. This means that the hours 23 and 0 are close to each other and not far apart as they would be if we encoded them as 23 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_timestamp_to_cyclical_features(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', utc=True)\n",
    "\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['minute'] = df['timestamp'].dt.minute\n",
    "    df['second'] = df['timestamp'].dt.second\n",
    "    \n",
    "    def encode_cyclical_feature(feature, value):\n",
    "        return np.sin(2 * np.pi * feature / value), np.cos(2 * np.pi * feature / value)\n",
    "\n",
    "    df['month_sin'], df['month_cos'] = encode_cyclical_feature(df['month'], 12)\n",
    "    df['day_sin'], df['day_cos'] = encode_cyclical_feature(df['day_of_week'], 7)\n",
    "    df['hour_sin'], df['hour_cos'] = encode_cyclical_feature(df['hour'], 24)\n",
    "    df['minute_sin'], df['minute_cos'] = encode_cyclical_feature(df['minute'], 60)\n",
    "    df['second_sin'], df['second_cos'] = encode_cyclical_feature(df['second'], 60)\n",
    "\n",
    "    df['unix_time'] = df['timestamp'].astype('int64') // 10**9\n",
    "\n",
    "    return df.drop(columns=['timestamp', 'month', 'day_of_week', 'hour', 'minute', 'second']) # Remove original timestamp and intermediate components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [encode_timestamp_to_cyclical_features(group) for group in X_train]\n",
    "X_val = [encode_timestamp_to_cyclical_features(group) for group in X_val]\n",
    "X_test = [encode_timestamp_to_cyclical_features(group) for group in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos', 'unix_time']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now left with `machine_shdr_execution` and `Machine_state_machine`. We will encode these columns by using them as an embedding layer in our model. This will allow the model to learn the relationship between the different machines and machine states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unqiue_executions = [\n",
    "    'ACTIVE',\n",
    "    'FEED_HOLD',\n",
    "    'INTERRUPTED',\n",
    "    'OPTIONAL_STOP',\n",
    "    'PROGRAM_STOPPED',\n",
    "    'PROGRAM_STOPPED\\r',\n",
    "    'READY',\n",
    "    'STOPPED',\n",
    "    'UNAVAILABLE',\n",
    "    'WAIT',\n",
    "    'PROGRAM_COMPLETED',\n",
    "]\n",
    "unqiue_executions_to_int = {execution: idx for idx, execution in enumerate(unqiue_executions)}\n",
    "unqiue_executions_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the machines we have choosen, based on all execution timeseries data, these are all unique values. The code for this was written in a temporary test file and is not included in this notebook. The code is as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "unique_execution_values = set()\n",
    "for machine in machine_external_ids:\n",
    "    exe = client.time_series.data.retrieve(external_id=f'{machine}_shdr_execution', limit=None).to_pandas()\n",
    "    unique_execution_values.update(exe[f'{machine}_shdr_execution'].unique())\n",
    "\n",
    "unique_execution_values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_execution_feature(df):\n",
    "    df['machine_shdr_execution'] = df['machine_shdr_execution'].map(unqiue_executions_to_int).astype(int)\n",
    "    return df\n",
    "\n",
    "X_train = [encode_execution_feature(group) for group in X_train]\n",
    "X_val = [encode_execution_feature(group) for group in X_val]\n",
    "X_test = [encode_execution_feature(group) for group in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[28]['machine_shdr_execution'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = [\n",
    "    'INCYCLE',\n",
    "    'IDLE',\n",
    "    'MANUAL MODE',\n",
    "    'POWER OFF',\n",
    "    'CAM CYCLE',\n",
    "    'MDI MODE',\n",
    "    'MDI CYCLE',\n",
    "    'FEEDHOLD',\n",
    "    'PROGRAM STOP',\n",
    "    'M0',\n",
    "    'ESTOP',\n",
    "    'ALARM',\n",
    "    'OPTIONAL STOP'\n",
    "]\n",
    "unique_state_to_int = {state: idx for idx, state in enumerate(unique_states)}\n",
    "\n",
    "def encode_state_feature(df):\n",
    "    df['Machine_state_machine'] = df['Machine_state_machine'].map(unique_state_to_int).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [encode_state_feature(group) for group in X_train]\n",
    "X_val = [encode_state_feature(group) for group in X_val]\n",
    "X_test = [encode_state_feature(group) for group in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[4]['Machine_state_machine'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now encoded all the categorical features and can proceed to impute the missing values and scale the data.\n",
    "\n",
    "We will start by decalring two lists, one for the numerical features that we will scale and one for the categorical features that will be used as an embedding layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = [\n",
    "    'machine_shdr_Zpos', \n",
    "    'machine_shdr_Fact_Numeric',\n",
    "    'machine_shdr_Frapidovr_Numeric', \n",
    "    'machine_shdr_Xpos',\n",
    "    'machine_shdr_Zfrt_Numeric', \n",
    "    'machine_shdr_Cpos',\n",
    "    'machine_shdr_Tool_group_Numeric', \n",
    "    'machine_shdr_Zabs',\n",
    "    'machine_shdr_Cload', \n",
    "    'machine_shdr_S2rpm_Numeric',\n",
    "    'accumulated_workorder_downtime_machine', \n",
    "    'accumulated_uptime_machine',\n",
    "    'machine_shdr_total_time_Numeric', \n",
    "    'machine_shdr_sequenceNum_Numeric',\n",
    "    'machine_shdr_Xfrt_Numeric', \n",
    "    'machine_shdr_auto_time_Numeric',\n",
    "    'machine_shdr_R172_Numeric', \n",
    "    'machine_shdr_Xload_Numeric',\n",
    "    'machine_shdr_Wfrt_Numeric', \n",
    "    'accumulated_workorder_uptime_machine',\n",
    "    'machine_shdr_Tool_number_Numeric', \n",
    "    'accumulated_downtime_machine', \n",
    "    'machine_shdr_Zload_Numeric',\n",
    "    'machine_shdr_Yfrt_Numeric', \n",
    "    'machine_shdr_Fovr_Numeric',\n",
    "    'machine_shdr_Yload_Numeric', \n",
    "    'machine_shdr_S2temp_Numeric',\n",
    "    'machine_shdr_Sovr_Numeric', \n",
    "    'machine_shdr_Xabs', \n",
    "    'machine_shdr_Bload', \n",
    "    'machine_shdr_Yabs',\n",
    "    'machine_shdr_Ypos', \n",
    "    'machine_shdr_S2load_Numeric',\n",
    "    'machine_shdr_cut_time_Numeric', \n",
    "    'machine_shdr_Wload_Numeric',\n",
    "    'machine_shdr_Srpm_Numeric', \n",
    "    'machine_shdr_Bpos',\n",
    "    'machine_shdr_Sload_Numeric', \n",
    "    'machine_shdr_Stemp_Numeric', \n",
    "    'unix_time'   \n",
    "]\n",
    "\n",
    "embedding_columns = ['Machine_state_machine', 'machine_shdr_execution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_columns = [col for col in X_train[0].columns if col not in cols_to_scale + embedding_columns]\n",
    "other_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_before_scaling = pd.concat(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_concat_before_scaling[cols_to_scale])  # Fitting only on training data\n",
    "\n",
    "def scale_group(df):    \n",
    "    df[cols_to_scale] = scaler.transform(df[cols_to_scale])\n",
    "    return df\n",
    "\n",
    "X_train = [scale_group(group) for group in X_train]\n",
    "X_val = [scale_group(group) for group in X_val]\n",
    "X_test = [scale_group(group) for group in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_after_scaling = pd.concat(X_train, axis=0)\n",
    "X_train_concat_after_scaling[cols_to_scale].describe().loc[['mean', 'std']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left to do now is to seperate the samples into the embedding features and the rest of the features. Then we need to convert the data to numpy arrays and save the pre-processed data to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = [group[cols_to_scale + other_columns].values for group in X_train]\n",
    "X_train_cat = [group[embedding_columns].values for group in X_train]\n",
    "\n",
    "X_val_num = [group[cols_to_scale + other_columns].values for group in X_val]\n",
    "X_val_cat = [group[embedding_columns].values for group in X_val]\n",
    "\n",
    "X_test_num = [group[cols_to_scale + other_columns].values for group in X_test]\n",
    "X_test_cat = [group[embedding_columns].values for group in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num[0], X_train_cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num[0].shape, X_train_cat[0].shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_num[0]), type(X_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good and we can now proceed to save all preprocessed arrays and any needed objects (like the scaler and numerical imputer) to a .pt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(root, 'project/data/')\n",
    "\n",
    "torch.save({\n",
    "    'X_train_num': X_train_num,\n",
    "    'X_train_cat': X_train_cat,\n",
    "    'y_train': y_train,\n",
    "    'X_val_num': X_val_num,\n",
    "    'X_val_cat': X_val_cat,\n",
    "    'y_val': y_val,\n",
    "    'X_test_num': X_test_num,\n",
    "    'X_test_cat': X_test_cat,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "}, data_folder + 'preprocessed_lstm_data.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
