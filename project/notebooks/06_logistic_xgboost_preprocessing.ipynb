{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Linear  Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will continue to pre-process our splitted data, to prepare it for a linear logistic regression model that will act as our benchmark and an XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Scaling is not performed here, as it is done in the python files for both models, using `sklearn.preprocessing.StandardScaler` inside the `sklearn.pipeline.Pipeline` object. This is done to avoid data leakage, as the scaler is fitted on the training data and then used to transform both the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Making the project modules available in the notebook\n",
    "root = os.path.abspath(os.path.join('../..'))\n",
    "if root not in sys.path: sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(os.path.join(root, 'project/data/splitted_data.pt'))\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_dict['X_train']\n",
    "X_val = data_dict['X_val']\n",
    "X_test = data_dict['X_test']\n",
    "\n",
    "y_train = data_dict['y_train']\n",
    "y_val = data_dict['y_val']\n",
    "y_test = data_dict['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'machine_shdr_execution': [\n",
    "        'ACTIVE',\n",
    "        'FEED_HOLD',\n",
    "        'INTERRUPTED',\n",
    "        'OPTIONAL_STOP',\n",
    "        'PROGRAM_STOPPED',\n",
    "        'PROGRAM_STOPPED\\r',\n",
    "        'READY',\n",
    "        'STOPPED',\n",
    "        'UNAVAILABLE',\n",
    "        'WAIT',\n",
    "        'PROGRAM_COMPLETED',\n",
    "    ],\n",
    "    'Machine_state_machine': [\n",
    "        'INCYCLE',\n",
    "        'IDLE',\n",
    "        'MANUAL MODE',\n",
    "        'POWER OFF',\n",
    "        'CAM CYCLE',\n",
    "        'MDI MODE',\n",
    "        'MDI CYCLE',\n",
    "        'FEEDHOLD',\n",
    "        'PROGRAM STOP',\n",
    "        'M0',\n",
    "        'ESTOP',\n",
    "        'ALARM',\n",
    "        'OPTIONAL STOP'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categorical_features(sensor_data, column_name):\n",
    "    counts = {}\n",
    "    total = len(sensor_data)\n",
    "    for cat in vocab[column_name]:\n",
    "        counts[cat] = 0\n",
    "    for val in sensor_data:\n",
    "        counts[val] += 1\n",
    "    return [counts[cat] / total for cat in vocab[column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_window(sub_df):\n",
    "    features = []\n",
    "    names = []\n",
    "    \n",
    "    # numerical features (flaot64)\n",
    "    numeric_columns = sub_df.select_dtypes(include=['float64']).columns\n",
    "    for column in numeric_columns:\n",
    "        sensor_data = sub_df[column].values\n",
    "        features.extend([\n",
    "            np.mean(sensor_data),\n",
    "            np.std(sensor_data),\n",
    "            np.min(sensor_data),\n",
    "            np.max(sensor_data), \n",
    "        ])\n",
    "        names.extend([\n",
    "            f'{column}_mean',\n",
    "            f'{column}_std',\n",
    "            f'{column}_min',\n",
    "            f'{column}_max'\n",
    "        ])\n",
    "        \n",
    "    # numerical features (int64) \n",
    "    int_columns = sub_df.select_dtypes(include=['int64']).columns\n",
    "    for column in int_columns:\n",
    "        sensor_data = sub_df[column].values\n",
    "        features.extend([\n",
    "            sensor_data[0],\n",
    "        ])\n",
    "        names.append(f'{column}_first')\n",
    "        \n",
    "    \n",
    "    # categorical features\n",
    "    cat_columns = sub_df.select_dtypes(include=['object']).columns\n",
    "    for column in cat_columns:\n",
    "        sensor_data = sub_df[column].values\n",
    "        if column == 'timestamp':\n",
    "            downtime_duration = pd.to_datetime(sensor_data[-1]) - pd.to_datetime(sensor_data[0])\n",
    "            features.extend([\n",
    "                downtime_duration.total_seconds(),\n",
    "            ])\n",
    "            names.append('downtime_duration')\n",
    "        else:\n",
    "            features.extend([\n",
    "                *extract_categorical_features(sensor_data, column)\n",
    "            ])\n",
    "            names.extend([f'{column}_{cat}' for cat in vocab[column]])\n",
    "            \n",
    "    return np.array(features), names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features, feature_names = compress_window(X_train[0]) # initialize the feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features, _  = zip(*[compress_window(sub_df) for sub_df in X_train])\n",
    "X_val_features, _ = zip(*[compress_window(sub_df) for sub_df in X_val])\n",
    "X_test_features, _ = zip(*[compress_window(sub_df) for sub_df in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features_array = np.array(X_train_features)\n",
    "\n",
    "downtime_index = feature_names.index('downtime_duration')\n",
    "downtime_duration_feature = X_train_features_array[:, downtime_index]\n",
    "\n",
    "correlation = np.corrcoef(downtime_duration_feature, y_train)[0, 1]\n",
    "print(f'Correlation between downtime_duration and target label: {correlation:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = np.array(X_train_features)\n",
    "X_val_features = np.array(X_val_features)\n",
    "X_test_features = np.array(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features.shape, X_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features.shape, X_val_features.shape, X_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'X_train': X_train_features,\n",
    "    'y_train': y_train,\n",
    "    'X_val': X_val_features,\n",
    "    'y_val': y_val,\n",
    "    'X_test': X_test_features,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': feature_names,\n",
    "    # 'scaler': scaler\n",
    "}, os.path.join(root, 'project/data/logistic_xgboost_data.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
